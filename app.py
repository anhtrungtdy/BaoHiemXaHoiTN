import streamlit as st
import google.generativeai as genai
import os
from PyPDF2 import PdfReader
from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
import time

# --- C√ÅC H√ÄM X·ª¨ L√ù ---

def get_text_from_files(files):
    """
    ƒê·ªçc v√† tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ danh s√°ch c√°c t·ªáp ƒë∆∞·ª£c t·∫£i l√™n (PDF, DOCX).
    """
    text = ""
    for file in files:
        # L·∫•y t√™n file ƒë·ªÉ ki·ªÉm tra ƒë·ªãnh d·∫°ng
        file_name = file.name
        st.write(f"ƒêang x·ª≠ l√Ω file: {file_name}")
        if file_name.endswith(".pdf"):
            try:
                pdf_reader = PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() or ""
            except Exception as e:
                st.error(f"L·ªói khi ƒë·ªçc file PDF {file_name}: {e}")
        elif file_name.endswith(".docx"):
            try:
                doc = Document(file)
                for para in doc.paragraphs:
                    text += para.text + "\n"
            except Exception as e:
                st.error(f"L·ªói khi ƒë·ªçc file DOCX {file_name}: {e}")
    return text

def get_text_chunks(text):
    """
    Chia vƒÉn b·∫£n ƒë·∫ßu v√†o th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n (chunks).
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000, 
        chunk_overlap=1000
    )
    chunks = text_splitter.split_text(text)
    return chunks

def get_vector_store(text_chunks):
    """
    Chuy·ªÉn ƒë·ªïi c√°c ƒëo·∫°n vƒÉn b·∫£n th√†nh vector v√† l∆∞u tr·ªØ b·∫±ng FAISS.
    S·ª≠ d·ª•ng session state ƒë·ªÉ l∆∞u tr·ªØ, tr√°nh t·ªën k√©m chi ph√≠ API.
    """
    try:
        # Kh·ªüi t·∫°o m√¥ h√¨nh embeddings c·ªßa Google
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        # T·∫°o c∆° s·ªü d·ªØ li·ªáu vector t·ª´ c√°c chunks
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        
        # L∆∞u v√†o session state ƒë·ªÉ t√°i s·ª≠ d·ª•ng
        st.session_state.vector_store = vector_store
        st.success("ƒê√£ vector h√≥a v√† l∆∞u tr·ªØ tri th·ª©c th√†nh c√¥ng!")

    except Exception as e:
        st.error(f"L·ªói khi t·∫°o vector store: {e}")
        st.error("Nguy√™n nh√¢n c√≥ th·ªÉ do API Key kh√¥ng h·ª£p l·ªá ho·∫∑c ch∆∞a ƒë∆∞·ª£c c·∫•p quy·ªÅn. H√£y th·ª≠ l·∫°i v·ªõi key kh√°c.")

def get_conversational_chain():
    """
    T·∫°o m·ªôt chu·ªói h·ªèi ƒë√°p (QA chain) v·ªõi prompt t√πy ch·ªânh.
    """
    prompt_template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c,
    d·ª±a ho√†n to√†n v√†o n·ªôi dung trong "ng·ªØ c·∫£nh" ƒë∆∞·ª£c cung c·∫•p.

    H√£y l√†m theo c√°c quy t·∫Øc sau:
    1. Ph√¢n t√≠ch k·ªπ c√¢u h·ªèi v√† ng·ªØ c·∫£nh tr∆∞·ªõc khi tr·∫£ l·ªùi.
    2. Tr√≠ch xu·∫•t t·∫•t c·∫£ th√¥ng tin li√™n quan t·ª´ ng·ªØ c·∫£nh ƒë·ªÉ x√¢y d·ª±ng c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß nh·∫•t.
    3. N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i r√µ: "R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin tr·∫£ l·ªùi cho c√¢u h·ªèi n√†y trong t√†i li·ªáu b·∫°n ƒë√£ cung c·∫•p."
    4. Tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c b·ªãa ƒë·∫∑t th√¥ng tin ho·∫∑c s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i ng·ªØ c·∫£nh.
    5. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng, m·∫°ch l·∫°c b·∫±ng ti·∫øng Vi·ªát.

    Ng·ªØ c·∫£nh:
    {context}

    C√¢u h·ªèi:
    {question}

    C√¢u tr·∫£ l·ªùi chi ti·∫øt:
    """
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh Gemini-Pro
    model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)
    
    # T·∫°o prompt v√† chu·ªói QA
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    return chain

def handle_user_input(user_question):
    """
    X·ª≠ l√Ω c√¢u h·ªèi, t√¨m ki·∫øm v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh.
    """
    if "vector_store" not in st.session_state or st.session_state.vector_store is None:
        st.warning("Vui l√≤ng t·∫£i l√™n v√† x·ª≠ l√Ω t√†i li·ªáu tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.")
        return

    try:
        with st.spinner("AI ƒëang suy nghƒ©..."):
            # T√¨m c√°c t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu vector
            docs = st.session_state.vector_store.similarity_search(user_question, k=5)
            
            # L·∫•y chu·ªói h·ªèi ƒë√°p
            chain = get_conversational_chain()
            
            # Ch·∫°y chu·ªói v√† nh·∫≠n k·∫øt qu·∫£
            response = chain(
                {"input_documents": docs, "question": user_question},
                return_only_outputs=True
            )
            
            # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
            st.session_state.chat_history.append(("user", user_question))
            st.session_state.chat_history.append(("bot", response["output_text"]))

            # Hi·ªÉn th·ªã l·∫°i to√†n b·ªô l·ªãch s·ª≠ chat
            display_chat_history()

    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")

def display_chat_history():
    """
    Hi·ªÉn th·ªã l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán.
    """
    for role, message in st.session_state.chat_history:
        if role == "user":
            with st.chat_message("user", avatar="üë§"):
                st.write(message)
        else:
            with st.chat_message("assistant", avatar="ü§ñ"):
                st.write(message)

# --- GIAO DI·ªÜN ·ª®NG D·ª§NG STREAMLIT ---

def main():
    st.set_page_config(page_title="Chat V·ªõi D·ªØ Li·ªáu C·ªßa B·∫°n", page_icon="üìö")

    # Kh·ªüi t·∫°o session state n·∫øu ch∆∞a c√≥
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None

    # --- THANH SIDEBAR ---
    with st.sidebar:
        st.title("Thi·∫øt l·∫≠p ‚öôÔ∏è")
        st.markdown("Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi ·ª©ng d·ª•ng chat v·ªõi d·ªØ li·ªáu c√° nh√¢n, ƒë∆∞·ª£c cung c·∫•p b·ªüi Gemini AI.")

        # Nh·∫≠p API Key
        try:
            # C·ªë g·∫Øng l·∫•y key t·ª´ secrets c·ªßa Streamlit tr∆∞·ªõc
            api_key = st.secrets["GOOGLE_API_KEY"]
            st.success("ƒê√£ t·∫£i API Key t·ª´ secrets!", icon="‚úÖ")
        except:
            # N·∫øu kh√¥ng c√≥, y√™u c·∫ßu ng∆∞·ªùi d√πng nh·∫≠p
            api_key = st.text_input("Nh·∫≠p Google API Key c·ªßa b·∫°n:", type="password", help="L·∫•y key t·∫°i aistudio.google.com")
        
        if api_key:
            try:
                genai.configure(api_key=api_key)
            except Exception as e:
                st.error(f"API Key kh√¥ng h·ª£p l·ªá: {e}")


        st.subheader("Ngu·ªìn tri th·ª©c c·ªßa b·∫°n")
        uploaded_files = st.file_uploader(
            "T·∫£i l√™n c√°c t·ªáp PDF ho·∫∑c DOCX",
            accept_multiple_files=True,
            type=['pdf', 'docx']
        )

        if st.button("X√¢y d·ª±ng c∆° s·ªü tri th·ª©c"):
            if not api_key:
                st.warning("Vui l√≤ng nh·∫≠p Google API Key tr∆∞·ªõc.")
            elif not uploaded_files:
                st.warning("Vui l√≤ng t·∫£i l√™n √≠t nh·∫•t m·ªôt t·ªáp t√†i li·ªáu.")
            else:
                with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t."):
                    # 1. Tr√≠ch xu·∫•t vƒÉn b·∫£n
                    raw_text = get_text_from_files(uploaded_files)
                    
                    if raw_text:
                        # 2. Chia th√†nh c√°c chunks
                        text_chunks = get_text_chunks(raw_text)
                        
                        # 3. Vector h√≥a v√† l∆∞u tr·ªØ
                        get_vector_store(text_chunks)
                    else:
                        st.error("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ c√°c t·ªáp ƒë√£ t·∫£i l√™n.")

    # --- KHUNG CHAT CH√çNH ---
    st.header("Chat V·ªõi D·ªØ Li·ªáu C·ªßa B·∫°n ü§ñüìö")
    st.write("T·∫£i t√†i li·ªáu c·ªßa b·∫°n l√™n, x√¢y d·ª±ng c∆° s·ªü tri th·ª©c, v√† b·∫Øt ƒë·∫ßu h·ªèi ƒë√°p!")
    st.markdown("---")

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat ƒë√£ c√≥
    display_chat_history()

    # √î nh·∫≠p c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
    user_question = st.chat_input("ƒê·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu c·ªßa b·∫°n...")
    if user_question:
        handle_user_input(user_question)

if __name__ == "__main__":
    main()
